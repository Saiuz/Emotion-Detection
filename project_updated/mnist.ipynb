{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTES\n",
    "#normalize images by deviding by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "def mass_resize(images, size=(128,128)):\n",
    "    arr = np.zeros((len(images),size[0],size[1]))\n",
    "    for i,im in enumerate(images):\n",
    "        arr[i] = misc.imresize(im,size)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ablac\\miniconda3\\envs\\expressions\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist =  tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "size = (128,128)\n",
    "\n",
    "x_train = mass_resize(x_train,size)\n",
    "x_test = mass_resize(x_test,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "X,Y = x_train,y_train\n",
    "X,Y = shuffle(X,Y)\n",
    "imshow(X[0])\n",
    "\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None,size[0],size[1]),name='x')\n",
    "y = tf.placeholder(dtype=tf.uint8, shape=(None),name=\"y\")\n",
    "lr = tf.placeholder(dtype=tf.float32, shape=[], name=\"lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = tf.one_hot(y,num_classes)\n",
    "reshaped = tf.reshape(x,[-1,size[0],size[1],1])\n",
    "\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch,128,128,1\n",
    "\n",
    "#first convolution\n",
    "conv = tf.contrib.layers.conv2d(\n",
    "    reshaped,\n",
    "    32,\n",
    "    (9,9),\n",
    "    stride=(1,1),\n",
    "    padding='VALID',\n",
    "    weights_initializer=tf.initializers.random_normal)\n",
    "#batch,120,120,32\n",
    "\n",
    "#subsample\n",
    "pool = tf.contrib.layers.max_pool2d(\n",
    "    conv,\n",
    "    (4,4),\n",
    "    stride=(4,4))\n",
    "#batch,30,30,32\n",
    "\n",
    "#second convolution\n",
    "conv2 = tf.contrib.layers.conv2d(\n",
    "    pool,\n",
    "    64,\n",
    "   (11,11),\n",
    "    stride=(1,1),\n",
    "    padding='VALID',\n",
    "    weights_initializer=tf.initializers.random_normal)\n",
    "#batch,20,20,64\n",
    "\n",
    "#subsample\n",
    "pool2 = tf.contrib.layers.max_pool2d(\n",
    "    conv2,\n",
    "    (4,4),\n",
    "    stride=(4,4))\n",
    "#batch,5,5,64\n",
    "\n",
    "#flatten\n",
    "flat = tf.contrib.layers.flatten(pool2)\n",
    "#batch,1600\n",
    "\n",
    "#fully connected\n",
    "fc = tf.contrib.layers.fully_connected(\n",
    "    flat,\n",
    "    1056,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    weights_initializer=tf.initializers.random_normal)\n",
    "#batch,256\n",
    "\n",
    "#logits = slim.fully_connected(flat, 10, activation_fn=None, normalizer_fn=None)\n",
    "logits = tf.contrib.layers.fully_connected(\n",
    "    fc,\n",
    "    num_classes,\n",
    "    activation_fn=None)\n",
    "\n",
    "prediction = tf.nn.softmax(logits,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=one_hot)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss_op)\n",
    "correct_pred = tf.equal(tf.argmax(prediction,1), tf.argmax(one_hot,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \tbatch: 100\ttrain accuracy: 0.582\tloss: 259204.375\tval acc: 0.582\n",
      "epoch 0 \tbatch: 200\ttrain accuracy: 0.7578\tloss: 119277.6172\tval acc: 0.7578\n",
      "Epoch 0 completed out of 40 loss: 103922787.9140625\n",
      "Test Accuracy: 0.77\n",
      "epoch 1 \tbatch: 100\ttrain accuracy: 0.8516\tloss: 75840.9453\tval acc: 0.8516\n",
      "epoch 1 \tbatch: 200\ttrain accuracy: 0.8555\tloss: 56840.7617\tval acc: 0.8555\n",
      "Epoch 1 completed out of 40 loss: 16283128.296875\n",
      "Test Accuracy: 0.87\n",
      "epoch 2 \tbatch: 100\ttrain accuracy: 0.9141\tloss: 32574.7969\tval acc: 0.9141\n",
      "epoch 2 \tbatch: 200\ttrain accuracy: 0.9141\tloss: 33002.3906\tval acc: 0.9141\n",
      "Epoch 2 completed out of 40 loss: 10292966.097167969\n",
      "Test Accuracy: 0.91\n",
      "epoch 3 \tbatch: 100\ttrain accuracy: 0.9219\tloss: 29874.1016\tval acc: 0.9219\n",
      "epoch 3 \tbatch: 200\ttrain accuracy: 0.9336\tloss: 36635.1172\tval acc: 0.9336\n",
      "Epoch 3 completed out of 40 loss: 7654404.576171875\n",
      "Test Accuracy: 0.9\n",
      "epoch 4 \tbatch: 100\ttrain accuracy: 0.8906\tloss: 62628.9922\tval acc: 0.8906\n",
      "epoch 4 \tbatch: 200\ttrain accuracy: 0.9375\tloss: 20226.1602\tval acc: 0.9375\n",
      "Epoch 4 completed out of 40 loss: 6046130.1630859375\n",
      "Test Accuracy: 0.92\n",
      "epoch 5 \tbatch: 100\ttrain accuracy: 0.9648\tloss: 9959.0117\tval acc: 0.9648\n",
      "epoch 5 \tbatch: 200\ttrain accuracy: 0.9062\tloss: 32716.6289\tval acc: 0.9062\n",
      "Epoch 5 completed out of 40 loss: 5056705.494628906\n",
      "Test Accuracy: 0.93\n",
      "epoch 6 \tbatch: 100\ttrain accuracy: 0.9648\tloss: 9662.7129\tval acc: 0.9648\n",
      "epoch 6 \tbatch: 200\ttrain accuracy: 0.9688\tloss: 8812.1553\tval acc: 0.9688\n",
      "Epoch 6 completed out of 40 loss: 4262253.981201172\n",
      "Test Accuracy: 0.93\n",
      "epoch 7 \tbatch: 100\ttrain accuracy: 0.9609\tloss: 18369.4551\tval acc: 0.9609\n",
      "epoch 7 \tbatch: 200\ttrain accuracy: 0.9414\tloss: 19085.4453\tval acc: 0.9414\n",
      "Epoch 7 completed out of 40 loss: 3662838.6936035156\n",
      "Test Accuracy: 0.93\n",
      "epoch 8 \tbatch: 100\ttrain accuracy: 0.9531\tloss: 15322.1562\tval acc: 0.9531\n",
      "epoch 8 \tbatch: 200\ttrain accuracy: 0.9609\tloss: 7861.9214\tval acc: 0.9609\n",
      "Epoch 8 completed out of 40 loss: 3232013.452758789\n",
      "Test Accuracy: 0.92\n",
      "epoch 9 \tbatch: 100\ttrain accuracy: 0.9688\tloss: 7455.3765\tval acc: 0.9688\n",
      "epoch 9 \tbatch: 200\ttrain accuracy: 0.9531\tloss: 15038.0596\tval acc: 0.9531\n",
      "Epoch 9 completed out of 40 loss: 2829762.462890625\n",
      "Test Accuracy: 0.93\n",
      "epoch 10 \tbatch: 100\ttrain accuracy: 0.9727\tloss: 6718.0342\tval acc: 0.9727\n",
      "epoch 10 \tbatch: 200\ttrain accuracy: 0.9727\tloss: 12143.8906\tval acc: 0.9727\n",
      "Epoch 10 completed out of 40 loss: 2513351.9204101562\n",
      "Test Accuracy: 0.95\n",
      "epoch 11 \tbatch: 100\ttrain accuracy: 0.9609\tloss: 13330.4385\tval acc: 0.9609\n",
      "epoch 11 \tbatch: 200\ttrain accuracy: 0.957\tloss: 14995.4951\tval acc: 0.957\n",
      "Epoch 11 completed out of 40 loss: 2208643.364501953\n",
      "Test Accuracy: 0.94\n",
      "epoch 12 \tbatch: 100\ttrain accuracy: 0.9766\tloss: 5232.6929\tval acc: 0.9766\n",
      "epoch 12 \tbatch: 200\ttrain accuracy: 0.9766\tloss: 5926.8247\tval acc: 0.9766\n",
      "Epoch 12 completed out of 40 loss: 1995555.861694336\n",
      "Test Accuracy: 0.95\n",
      "epoch 13 \tbatch: 100\ttrain accuracy: 0.9883\tloss: 760.873\tval acc: 0.9883\n",
      "epoch 13 \tbatch: 200\ttrain accuracy: 0.9805\tloss: 5933.8594\tval acc: 0.9805\n",
      "Epoch 13 completed out of 40 loss: 1792989.2253417969\n",
      "Test Accuracy: 0.96\n",
      "epoch 14 \tbatch: 100\ttrain accuracy: 0.9766\tloss: 6182.1582\tval acc: 0.9766\n",
      "epoch 14 \tbatch: 200\ttrain accuracy: 0.9805\tloss: 6602.2354\tval acc: 0.9805\n",
      "Epoch 14 completed out of 40 loss: 1627647.5150146484\n",
      "Test Accuracy: 0.97\n",
      "epoch 15 \tbatch: 100\ttrain accuracy: 0.9766\tloss: 9050.5762\tval acc: 0.9766\n",
      "epoch 15 \tbatch: 200\ttrain accuracy: 0.9648\tloss: 16571.1113\tval acc: 0.9648\n",
      "Epoch 15 completed out of 40 loss: 1428729.47265625\n",
      "Test Accuracy: 0.96\n",
      "epoch 16 \tbatch: 100\ttrain accuracy: 0.9844\tloss: 1888.3633\tval acc: 0.9844\n",
      "epoch 16 \tbatch: 200\ttrain accuracy: 0.9727\tloss: 4966.1387\tval acc: 0.9727\n",
      "Epoch 16 completed out of 40 loss: 1307990.8171386719\n",
      "Test Accuracy: 0.97\n",
      "epoch 17 \tbatch: 100\ttrain accuracy: 0.9883\tloss: 2750.542\tval acc: 0.9883\n",
      "epoch 17 \tbatch: 200\ttrain accuracy: 0.9922\tloss: 761.4653\tval acc: 0.9922\n",
      "Epoch 17 completed out of 40 loss: 1186639.423461914\n",
      "Test Accuracy: 0.98\n",
      "epoch 18 \tbatch: 100\ttrain accuracy: 0.9766\tloss: 3854.7517\tval acc: 0.9766\n",
      "epoch 18 \tbatch: 200\ttrain accuracy: 0.9883\tloss: 2860.7681\tval acc: 0.9883\n",
      "Epoch 18 completed out of 40 loss: 1098297.98828125\n",
      "Test Accuracy: 0.97\n",
      "epoch 19 \tbatch: 100\ttrain accuracy: 0.9883\tloss: 813.8865\tval acc: 0.9883\n",
      "epoch 19 \tbatch: 200\ttrain accuracy: 0.9727\tloss: 3833.6018\tval acc: 0.9727\n",
      "Epoch 19 completed out of 40 loss: 990544.1585083008\n",
      "Test Accuracy: 0.97\n",
      "epoch 20 \tbatch: 100\ttrain accuracy: 0.9805\tloss: 8857.0889\tval acc: 0.9805\n",
      "epoch 20 \tbatch: 200\ttrain accuracy: 0.9922\tloss: 1773.0166\tval acc: 0.9922\n",
      "Epoch 20 completed out of 40 loss: 935294.0977478027\n",
      "Test Accuracy: 0.97\n",
      "epoch 21 \tbatch: 100\ttrain accuracy: 0.9766\tloss: 2072.1396\tval acc: 0.9766\n",
      "epoch 21 \tbatch: 200\ttrain accuracy: 0.9805\tloss: 2688.1243\tval acc: 0.9805\n",
      "Epoch 21 completed out of 40 loss: 780740.3427429199\n",
      "Test Accuracy: 0.96\n",
      "epoch 22 \tbatch: 100\ttrain accuracy: 0.9844\tloss: 2796.5669\tval acc: 0.9844\n",
      "epoch 22 \tbatch: 200\ttrain accuracy: 0.9883\tloss: 2145.3716\tval acc: 0.9883\n",
      "Epoch 22 completed out of 40 loss: 783175.3378295898\n",
      "Test Accuracy: 0.97\n",
      "epoch 23 \tbatch: 100\ttrain accuracy: 0.9844\tloss: 1072.748\tval acc: 0.9844\n",
      "epoch 23 \tbatch: 200\ttrain accuracy: 0.9727\tloss: 2743.5142\tval acc: 0.9727\n",
      "Epoch 23 completed out of 40 loss: 667477.827911377\n",
      "Test Accuracy: 0.96\n",
      "epoch 24 \tbatch: 100\ttrain accuracy: 0.9883\tloss: 3060.8049\tval acc: 0.9883\n",
      "epoch 24 \tbatch: 200\ttrain accuracy: 0.9922\tloss: 585.2017\tval acc: 0.9922\n",
      "Epoch 24 completed out of 40 loss: 604791.7471008301\n",
      "Test Accuracy: 0.97\n",
      "epoch 25 \tbatch: 100\ttrain accuracy: 0.9883\tloss: 1569.0225\tval acc: 0.9883\n",
      "epoch 25 \tbatch: 200\ttrain accuracy: 0.9805\tloss: 3718.6538\tval acc: 0.9805\n",
      "Epoch 25 completed out of 40 loss: 606230.5386657715\n",
      "Test Accuracy: 0.97\n",
      "epoch 26 \tbatch: 100\ttrain accuracy: 0.9844\tloss: 1941.6008\tval acc: 0.9844\n",
      "epoch 26 \tbatch: 200\ttrain accuracy: 0.9727\tloss: 3872.4155\tval acc: 0.9727\n",
      "Epoch 26 completed out of 40 loss: 529703.825958252\n",
      "Test Accuracy: 0.97\n",
      "epoch 27 \tbatch: 100\ttrain accuracy: 0.9922\tloss: 412.7874\tval acc: 0.9922\n",
      "epoch 27 \tbatch: 200\ttrain accuracy: 0.9844\tloss: 4604.9448\tval acc: 0.9844\n",
      "Epoch 27 completed out of 40 loss: 493178.0159301758\n",
      "Test Accuracy: 0.97\n",
      "epoch 28 \tbatch: 100\ttrain accuracy: 0.9844\tloss: 1602.335\tval acc: 0.9844\n",
      "epoch 28 \tbatch: 200\ttrain accuracy: 0.9883\tloss: 1336.7732\tval acc: 0.9883\n",
      "Epoch 28 completed out of 40 loss: 412641.19818115234\n",
      "Test Accuracy: 0.98\n",
      "epoch 29 \tbatch: 100\ttrain accuracy: 0.9922\tloss: 1044.9934\tval acc: 0.9922\n",
      "epoch 29 \tbatch: 200\ttrain accuracy: 0.9961\tloss: 16.7749\tval acc: 0.9961\n",
      "Epoch 29 completed out of 40 loss: 414841.74365234375\n",
      "Test Accuracy: 0.97\n",
      "epoch 30 \tbatch: 100\ttrain accuracy: 0.9844\tloss: 2073.3562\tval acc: 0.9844\n",
      "epoch 30 \tbatch: 200\ttrain accuracy: 1.0\tloss: 0.0\tval acc: 1.0\n",
      "Epoch 30 completed out of 40 loss: 362637.2981414795\n",
      "Test Accuracy: 0.97\n",
      "epoch 31 \tbatch: 100\ttrain accuracy: 0.9922\tloss: 2186.1125\tval acc: 0.9922\n",
      "epoch 31 \tbatch: 200\ttrain accuracy: 1.0\tloss: 0.0\tval acc: 1.0\n",
      "Epoch 31 completed out of 40 loss: 318271.4432067871\n",
      "Test Accuracy: 0.97\n",
      "epoch 32 \tbatch: 100\ttrain accuracy: 0.9883\tloss: 722.1282\tval acc: 0.9883\n",
      "epoch 32 \tbatch: 200\ttrain accuracy: 0.9961\tloss: 368.5813\tval acc: 0.9961\n",
      "Epoch 32 completed out of 40 loss: 258792.64770507812\n",
      "Test Accuracy: 0.97\n",
      "epoch 33 \tbatch: 100\ttrain accuracy: 0.9961\tloss: 338.115\tval acc: 0.9961\n",
      "epoch 33 \tbatch: 200\ttrain accuracy: 0.9961\tloss: 270.7676\tval acc: 0.9961\n",
      "Epoch 33 completed out of 40 loss: 312007.82345581055\n",
      "Test Accuracy: 0.97\n",
      "epoch 34 \tbatch: 100\ttrain accuracy: 0.9883\tloss: 594.4373\tval acc: 0.9883\n",
      "epoch 34 \tbatch: 200\ttrain accuracy: 0.9805\tloss: 4493.6665\tval acc: 0.9805\n",
      "Epoch 34 completed out of 40 loss: 250194.76516723633\n",
      "Test Accuracy: 0.97\n",
      "epoch 35 \tbatch: 100\ttrain accuracy: 1.0\tloss: 0.0\tval acc: 1.0\n",
      "epoch 35 \tbatch: 200\ttrain accuracy: 0.9922\tloss: 663.0652\tval acc: 0.9922\n",
      "Epoch 35 completed out of 40 loss: 216805.54415893555\n",
      "Test Accuracy: 0.97\n",
      "epoch 36 \tbatch: 100\ttrain accuracy: 0.9961\tloss: 641.2451\tval acc: 0.9961\n",
      "epoch 36 \tbatch: 200\ttrain accuracy: 0.9961\tloss: 761.9561\tval acc: 0.9961\n",
      "Epoch 36 completed out of 40 loss: 223024.26258850098\n",
      "Test Accuracy: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37 \tbatch: 100\ttrain accuracy: 1.0\tloss: 0.0\tval acc: 1.0\n",
      "epoch 37 \tbatch: 200\ttrain accuracy: 0.9883\tloss: 461.4282\tval acc: 0.9883\n",
      "Epoch 37 completed out of 40 loss: 193967.28393554688\n",
      "Test Accuracy: 0.96\n",
      "epoch 38 \tbatch: 100\ttrain accuracy: 1.0\tloss: 0.0\tval acc: 1.0\n",
      "epoch 38 \tbatch: 200\ttrain accuracy: 0.9961\tloss: 873.7026\tval acc: 0.9961\n",
      "Epoch 38 completed out of 40 loss: 238248.2107849121\n",
      "Test Accuracy: 0.96\n",
      "epoch 39 \tbatch: 100\ttrain accuracy: 0.9961\tloss: 124.9678\tval acc: 0.9961\n",
      "epoch 39 \tbatch: 200\ttrain accuracy: 0.9961\tloss: 463.9555\tval acc: 0.9961\n",
      "Epoch 39 completed out of 40 loss: 151627.134973526\n",
      "Test Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 40\n",
    "learn_rate = .0001\n",
    "batch_size = 256\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tx,ty = shuffle(x_test,y_test)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_i = 1\n",
    "        #for X,Y in next_batch(x_train,y_train,batch_size):\n",
    "        X,Y = x_train,y_train\n",
    "        X,Y = shuffle(X,Y)\n",
    "        \n",
    "        for i in range(0,len(X),batch_size):\n",
    "            x_batch = X[i:i+batch_size]\n",
    "            y_batch = Y[i:i+batch_size]\n",
    "            _,c = sess.run([optimizer,loss_op],feed_dict={x:x_batch,y:y_batch,lr:learn_rate})\n",
    "            epoch_loss += c\n",
    "            if batch_i % 100 == 0:\n",
    "                valacc = sess.run(accuracy,feed_dict={x:x_batch,y:y_batch})\n",
    "                loss, trainacc = sess.run([loss_op, accuracy], feed_dict={x:x_batch,y:y_batch})\n",
    "                print(\"epoch {} \\tbatch: {}\\ttrain accuracy: {}\\tloss: {}\\tval acc: {}\"\n",
    "                      .format(epoch,batch_i,round(float(trainacc),4),round(float(loss),4), round(float(valacc),4)))               \n",
    "                \n",
    "            batch_i += 1\n",
    "        print('Epoch', epoch, 'completed out of',epochs,'loss:',epoch_loss)\n",
    "            \n",
    "    \n",
    "        \n",
    "        print('Test Accuracy:',accuracy.eval({x:tx[:100], y:ty[:100]}))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "def neural_network_model(data):\n",
    "    hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "    hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "    hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "    output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                    'biases':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "\n",
    "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            batch_i = 1\n",
    "            epoch_loss = 0\n",
    "            #for X,Y in next_batch(x_train,y_train,batch_size):\n",
    "            X,Y = x_train,y_train\n",
    "            X,Y = shuffle(X,Y)\n",
    "\n",
    "            for i in range(0,len(X),batch_size):\n",
    "                x_batch = X[i:i+batch_size]\n",
    "                y_batch = Y[i:i+batch_size]\n",
    "                sess.run(optimizer,feed_dict={x:x_batch,y:y_batch,lr:learn_rate})\n",
    "                if batch_i % 10 == 0:\n",
    "                    valacc = sess.run(accuracy,feed_dict={x:x_test[:100],y:y_test[:100]})\n",
    "                    loss, trainacc = sess.run([loss_op, accuracy], feed_dict={x:x_batch,y:y_batch})\n",
    "                    print(\"epoch {} \\tbatch: {}\\ttrain accuracy: {}\\tloss: {}\\tval acc: {}\"\n",
    "                          .format(epoch,batch_i,round(float(trainacc),4),round(float(loss),4), round(float(valacc),4)))               \n",
    "\n",
    "                batch_i += 1\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
